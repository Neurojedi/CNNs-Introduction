{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8fe5cf4",
   "metadata": {
    "id": "e8fe5cf4"
   },
   "source": [
    "# Fundamentals of Convolutional Neural Networks 2 - Pooling and Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1ddcb",
   "metadata": {},
   "source": [
    "Welcome to the second notebook on Convolutional Neural Networks. In this notebook, we will explore Pooling and Dense Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17711d",
   "metadata": {
    "id": "6e17711d"
   },
   "source": [
    "## Pooling Layers\n",
    "\n",
    "We previously saw that we can increase the downsampling capability of our convolutional layer by increasing the stride parameter. Another way to add downsampling to our convolutional neural network, which is often used is the pooling operation. The pooling operation is quite easy to grasp since we are familiar with the concept of convolution now. In pooling layers, every neuron is connected to only a unique, limited number of the neurons in the previous layer (so these layers also have a receptive field just like convolutional layers). The neurons in the pooling layer have similar parameters to convolutional layers as well. However, neurons in pooling layers unlike the convolutional layers, use a hardcoded aggregation function such as max or min. Let's see an illustration of this operation.\n",
    "\n",
    "<img src=\"imgs/gif4.gif\" width=\"40%\">\n",
    "\n",
    "<a href=\"https://pub.towardsai.net/introduction-to-pooling-layers-in-cnn-dafe61eabe34\"> Source </a>\n",
    "\n",
    "The most commonly used two pooling techniques are illustrated above. Similar to the convolutional layers, we again slide a kernel over unique patches of the input feature map (observe that stride is set to 2x2, which indicates we use a kernel with a step size of 2 in both height and width axis), but this time instead of using a learnable kernel to transform local patches, we use a hardcoded operation either by calculating the max value of each patch or the average. Moreover, pooling layers generally work on each color channel separately and they only reduce the spatial dimensionality of the data.\n",
    "\n",
    "Let's use both of these techniques starting with `keras.layers.MaxPool2D()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5c57f",
   "metadata": {
    "id": "71d5c57f"
   },
   "source": [
    "Firstly, let's load Cifar10 dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f4e79",
   "metadata": {
    "id": "f47f4e79",
    "outputId": "6cd3bbd3-dc61-4943-fcb4-e2d19316bf5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19d587ede80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "sample=x_train[:2,:,:,:]\n",
    "plt.imshow(sample[0,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707d392",
   "metadata": {
    "id": "2707d392"
   },
   "source": [
    "Just as `keras.layers.Conv2D()` we can define the size, padding type and stride parameters of `tf.keras.layers.MaxPool2D()` but first let's start off with the default setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d6589",
   "metadata": {
    "id": "8a6d6589"
   },
   "outputs": [],
   "source": [
    "maxpool2D = tf.keras.layers.MaxPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0255c4c",
   "metadata": {
    "id": "e0255c4c"
   },
   "outputs": [],
   "source": [
    "output = maxpool2D(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23131d23",
   "metadata": {
    "id": "23131d23",
    "outputId": "963a19a9-5eef-46d4-a8c6-3b5a2900e69f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 16, 16, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce1bb6",
   "metadata": {
    "id": "7bce1bb6",
    "outputId": "2d158e38-d680-4125-b50b-7ba16abac2f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 15.5, 15.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,7))\n",
    "ax1.imshow(sample[0,:,:,:])\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(output[0,:,:,:])\n",
    "ax2.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9043933",
   "metadata": {
    "id": "d9043933"
   },
   "source": [
    "The default parameter choices of `tf.keras.layers.MaxPool2D()` is 2x2 kernel with a stride of two and valid padding. This default parameters reduced the size of our image two times in width and height. We can tune these parameters just as we tuned the parameters of `tf.keras.layers.Conv2D()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86d38f",
   "metadata": {
    "id": "4f86d38f"
   },
   "outputs": [],
   "source": [
    "maxpool2D = tf.keras.layers.MaxPool2D(pool_size=2,strides=2,padding=\"valid\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27bfcc",
   "metadata": {
    "id": "ad27bfcc"
   },
   "source": [
    "To see all the parameters that you can tune, check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c790bea",
   "metadata": {
    "id": "8c790bea"
   },
   "source": [
    "Let's move on to `tf.keras.layers.AveragePooling2D()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed61b5",
   "metadata": {
    "id": "baed61b5"
   },
   "outputs": [],
   "source": [
    "avgpool2D = tf.keras.layers.AveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01cf20",
   "metadata": {
    "id": "6b01cf20",
    "outputId": "f3ab7a19-78c5-4ae4-be84-abe4a00a5960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer average_pooling2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = avgpool2D(sample/255.0) # the data type should not be uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901afd14",
   "metadata": {
    "id": "901afd14",
    "outputId": "389bb2fe-0517-4e30-ebc8-1c5ce95c817f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 16, 16, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13ef92",
   "metadata": {
    "id": "3f13ef92",
    "outputId": "b0122701-5db4-461d-b75b-817759ae3ec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 15.5, 15.5, -0.5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,7))\n",
    "ax1.imshow(sample[0,:,:,:])\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(output[0,:,:,:])\n",
    "ax2.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7b207",
   "metadata": {
    "id": "dbe7b207"
   },
   "source": [
    "The default parameter choices of `tf.keras.layers.AveragePooling2D()` is the same as `tf.keras.layers.MaxPool2D()` and we can tune it in the same way. Furthermore, MaxPooling tends to work better in practice, as its ability to preserve the strongest features and omit the weak ones can filter the noise and provide a cleaner signal to successive layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686aefa",
   "metadata": {},
   "source": [
    "Documentation for AveragePooling2D: [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56331c",
   "metadata": {
    "id": "5f56331c"
   },
   "source": [
    "### The need of Downsampling and Pooling Layers\n",
    "\n",
    "\n",
    "Why do we need down-sampling or more specifically pooling layers?\n",
    "\n",
    "1. **To distill the high amount of information to reduce computational cost:** An image can be consisting of millions of pixels, which means we need to calculate millions of linear/non-linear operations in neurons. By using pooling layers, we can downsample the input and reduce the computational costs.\n",
    "\n",
    "\n",
    "2. **Increasing receptive field of successive CNN filters:** As you can see in our example above, after the pooling layer the input halved into 16x16, by keeping the same kernel size in our successive CNN layers we will let the layers filter information from increasingly larger receptive fields.\n",
    "\n",
    "\n",
    "3. **Invariance and Non-linearity in Pooling Layers:** While performing the max-pooling operation to a patch, it does not matter where exactly the highest value is, because the only thing that the aggregation function will care about is where the highest value is in that patch. Therefore, pooling layers also introduce invariance to small translations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97075758",
   "metadata": {
    "id": "97075758"
   },
   "source": [
    "<img src=\"imgs/image7.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa15f1",
   "metadata": {
    "id": "43fa15f1"
   },
   "source": [
    "<a href=\"https://www.deeplearningbook.org/contents/convnets.html\"> Source </a>\n",
    "\n",
    "Average pooling also adds a similar invariance (generally less than max-pooling). Adding more invariance can be beneficial in case of some classification problem, for instance when we are trying to classify by only caring whether a feature is present rather than exactly where it is (for instance, let's say we are doing a flower classification to recognize different types of flower). In such a case, pooling layers can help to blur the location information to some extent. However, making the model completely location-agnostic would require more data (data augmentation can be beneficial to force the model to learn location invariance). On the other hand, this invariance may not be always beneficial. For instance, when we are dealing with semantic segmentation, the relative location of instances would be an important feature to consider since we need to classify the pixels according to the position of the target object (if the target object is translated to right or left then the predicted object should be translated). In this case, we need equivariance. You can find more about equivariance in the further reading section. On the other hand, one other advantage of the max-pooling operation is that it also introduces some non-linearity whereas average pooling does not have that effect.\n",
    "\n",
    "Lastly, there is yet one pooling layer type to learn, namely global average pooling, which works similarly to average pooling but instead of taking averages on patches, it takes averages of each feature map. Then we can use the softmax layer or fully connected layers without flattening the feature maps. We will soon learn about this layer type as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c197d",
   "metadata": {
    "id": "809c197d"
   },
   "source": [
    "For learning more about invariance and equivariance, I recommend the following sources:\n",
    "1. [Deep Learning Equivariance and Invariance](https://www.doc.ic.ac.uk/~bkainz/teaching/DL/notes/equivariance.pdf)\n",
    "2. [Pooling in Convolutional Networks](https://cedar.buffalo.edu/~srihari/CSE676/9.3%20Pooling.pdf)\n",
    "3. [What is the benefit of using average pooling rather than max pooling?](https://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling)\n",
    "4. [Why is max pooling necessary in convolutional neural networks?](https://stats.stackexchange.com/questions/288261/why-is-max-pooling-necessary-in-convolutional-neural-networks)\n",
    "5. [How exactly does max pooling create translation invariance?](https://www.quora.com/How-exactly-does-max-pooling-create-translation-invariance)\n",
    "6. [Max Pooling in Convolutional Neural Network and Its Features](https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/)\n",
    "7. [Comprehensive Guide to Different Pooling Layers in Deep Learning](https://analyticsindiamag.com/comprehensive-guide-to-different-pooling-layers-in-deep-learning/)\n",
    "8. [Introduction To Pooling Layers In CNN](https://pub.towardsai.net/introduction-to-pooling-layers-in-cnn-dafe61eabe34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8f7b5",
   "metadata": {
    "id": "17b8f7b5"
   },
   "source": [
    "## Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3897c",
   "metadata": {
    "id": "08d3897c"
   },
   "source": [
    "In the first notebook, we learnt about the convolutional layers that filter the input and create feature maps. Then we talked about pooling layers that downsample the input to reduce computational cost in addition to its other advantages. In a typical convolutional neural network architecture, convolutional layers are followed by pooling layers and these layers are stacked together one after another several times to extract features effectively. The question is then, how can we use these extracted features in classification? Therein lies the need of using our old friends, Dense layers (at least for traditional ConvNets, later we will also see Fully Convolutional Neural Networks that do not include Dense Layers). \n",
    "\n",
    "\n",
    "<img src=\"imgs/web1.webp\" width=\"70%\">\n",
    "\n",
    "<a href=\"https://www.dominodatalab.com/blog/gpu-accelerated-convolutional-neural-networks-with-pytorch\"> Source </a>\n",
    "\n",
    "In this sense, we can say that the convolutional neural network has two main parts: the first part is the feature extractor consisting of convolutional layers and pooling layers, and the second part is the decision-making part which handles the classification. Now let's explore the components of this classification part starting with flatten layer.\n",
    "\n",
    "<img src=\"imgs/image8.png\" width=\"40%\">\n",
    "\n",
    "We previously used `tf.keras.layers.Flatten()` in my Deep Learning Fundamental notebooks and you can see the illustration above. As you may recall, it reshapes a tensor to have the same shape as the number of elements in the tensor, in other words, it converts a grid-shape structure (such as images or pooled feature maps in our case) into a 1-D vector since dense layers expect it that way. \n",
    "\n",
    "<img src=\"imgs/image9.png\" width=\"30%\">\n",
    "\n",
    "<a href=\"\"> Source </a>\n",
    "\n",
    "After the flattening, we pass the 1-D vector to the dense layers as represented in the above illustration. In the last layer, we again use the dense layers to give the final result. For a binary classification problem, we generally use one neuron with a sigmoid activation function which will output either 0 or 1 (assigning values lower than 0.5 to the negative class and the higher to the positive class). For multi-class classification problems, we need to use as many neurons as the number of classes where each neuron uses the softmax activation function to output probabilities. Then we can use the argmax operator to assign an instance to a class where the output is maximum given the instance. This case is illustrated in the first schematic of this section. We have three classes Car, Computer, and Dog, after we feed the instance to the neural network the first output neuron gives the highest value (0.82) then we can assign the instance to the Computer class using the argmax operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd5877",
   "metadata": {
    "id": "a7bd5877"
   },
   "source": [
    "### Differences between MLPs and CNNs\n",
    "\n",
    "**1. Computational costs:** The Convolutional Neural Networks extensively reduce the number of parameters that need to be tuned during training. Convolutional neural networks operate on small filters that share different weights (multiple filters) so that the output size is influenced by the size of the filter. Moreover, we also use pooling layers to further reduce the computation costs. However, dense layers operate on linear operations (the output of these linear operations generally is then fed to a non-linearity) to form a relationship between every input and every output in their layers. In other words, the number of parameters in dense layers is influenced by the input size (dimensionality of the data), whereas that is not the case for convolutional networks. Therefore, we can efficiently train convolutional neural networks without impacting the number of parameters that need to be tuned.\n",
    "\n",
    "<img src=\"imgs/gif5.gif\" width=\"40%\">\n",
    "\n",
    "<a href=\"\"> Source </a>\n",
    "\n",
    "Let's look at the illustration above. After flattening the 28x28 image, the 1-D input vector is fed to the neural network. The network then propagates the outputs to the successive layers and does the final classification in the output layer. Let's do a quick parameter calculation for this neural network with a hidden network size of 2. The first network after the input layer has 16 neurons, then the parameters of the first hidden layer would be 784x16+16 (input_shape*neurons+bias) = 12560. The second hidden layer will have 272 parameters and the last layer has 170. In total, we would have 13.002 parameters to tune. In real life, images have much higher resolution and we would need much deeper networks, this would blow up the number of parameters. Instead of using very deep dense layers, we can use the convolutional layers and pooling layers then add the dense layers for calculating the final classification results so that we can save a lot of computational resources.\n",
    "\n",
    "For learning more about parameter calculations: \n",
    "1. [Understanding and Calculating the number of Parameters in Convolution Neural Networks (CNNs)](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d)\n",
    "2.[How to Calculate the Number of Parameters in Keras Models](https://towardsdatascience.com/how-to-calculate-the-number-of-parameters-in-keras-models-710683dae0ca)\n",
    "\n",
    "**2. Local Information vs Global Information:**  Images are grid-type structures in which relative positions of pixels are quite important. As we previously discussed, Convolutional Neural Networks extract local features from the image. The extracted low-level features (edges, textures, and color gradients) can be used to localize more advanced features such as the presence of whiskers or the eye of a cat. Moreover, the filters respond to features whatever their location in the image. The filters in CNN layers, therefore, are able to exploit spatial information effectively. Fully Connected Layers, on the other hand, learn global information (patterns involving all the pixels). These global patterns do not lead to the strong inductive bias of CNNs which gives them several very important features such as being transition-invariant as well as learning spatial hierarchies. The main reason for the lack of spatial reasoning in Dense Layers is that when we flatten the vector we lose the spatial information since we combine all the pixel values without taking into account their original positions.\n",
    "\n",
    "\n",
    "<img src=\"imgs/gif6.gif\" width=\"60%\">\n",
    "\n",
    "<a href=\"\"> Source </a>\n",
    "\n",
    "See the illustration above, we can exploit the local patterns using convolutional layers and then use the extracted feature maps would provide us more information about the spatial information of the images even after we flatten the feature maps and feed them to fully connected layers, hence we could achieve better classification performance than directly feeding the images to a network of Fully Connected Layers. Besides, we would save many more computational resources.\n",
    "\n",
    "Finally, let's look at the illustration below to see the steps we have gone through up to now and then we will train our first CNN on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef46d6",
   "metadata": {
    "id": "26ef46d6"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"imgs/gif7.gif\" width=\"70%\">\n",
    "\n",
    "<a href=\"\"> Source </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fed0c",
   "metadata": {},
   "source": [
    "Now let's train our first CNN on MNIST Fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7436f25b",
   "metadata": {
    "id": "7436f25b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "(X_train, y_train), (X_valid, y_valid) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bd564a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7bd564a",
    "outputId": "b9277de3-0099-4540-cf52-05be65c6a3d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322081e6",
   "metadata": {
    "id": "322081e6"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(28,28,1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=7,activation='relu', padding=\"SAME\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=7,activation='relu', padding=\"SAME\"),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=7,activation='relu', padding=\"SAME\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=7,activation='relu', padding=\"SAME\"),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=7,activation='relu', padding=\"SAME\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a003f27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a003f27",
    "outputId": "dc727c45-7bb0-4981-d247-3cebf29716e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 28, 28, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        3200      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 128)       401536    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 128)       802944    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 256)         1605888   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 256)         3211520   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 3, 3, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,330,830\n",
      "Trainable params: 6,329,932\n",
      "Non-trainable params: 898\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9aa9d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe9aa9d6",
    "outputId": "93f6c274-01ca-4410-b5d9-298524055729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 60s 24ms/step - loss: 0.9274 - accuracy: 0.6666 - val_loss: 0.5812 - val_accuracy: 0.7498\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.5767 - accuracy: 0.7712 - val_loss: 0.4760 - val_accuracy: 0.8153\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.4903 - accuracy: 0.8168 - val_loss: 0.4276 - val_accuracy: 0.8385\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.4303 - accuracy: 0.8399 - val_loss: 0.3861 - val_accuracy: 0.8672\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.3753 - accuracy: 0.8714 - val_loss: 0.3729 - val_accuracy: 0.8687\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.3292 - accuracy: 0.8881 - val_loss: 0.3526 - val_accuracy: 0.8770\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.2909 - accuracy: 0.9006 - val_loss: 0.3375 - val_accuracy: 0.8747\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.2691 - accuracy: 0.9094 - val_loss: 0.2913 - val_accuracy: 0.9019\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.2441 - accuracy: 0.9175 - val_loss: 0.2616 - val_accuracy: 0.9093\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.2153 - accuracy: 0.9254 - val_loss: 0.2651 - val_accuracy: 0.9146\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c3035",
   "metadata": {
    "id": "a45c3035"
   },
   "source": [
    "General Reading Recommendations on CNNs:\n",
    "\n",
    "1. [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "2. [Gentle Dive into Math Behind Convolutional Neural Networks](https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9)\n",
    "3. [Convolution arithmetic tutorial](https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html)\n",
    "5. [Student Notes: Convolutional Neural Networks (CNN) Introduction](https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/)\n",
    "5. [A simple explanation to filters, stride and padding in CNN](https://shubhamchauhan125.medium.com/a-simple-explanation-to-filters-stride-and-padding-in-cnn-d0236d4a57ef)\n",
    "5. [What is the meaning of flattening step in a convolutional neural network?](https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network)\n",
    "6. [Computer Vision: Convolution Basics](https://towardsdatascience.com/computer-vision-convolution-basics-2d0ae3b79346)\n",
    "\n",
    "I also recommend watching the videos below:\n",
    "\n",
    "1. [Ch 9: Convolutional Networks](https://www.youtube.com/watch?v=Xogn6veSyxA)\n",
    "2. [How convolutional neural networks work, in depth](https://www.youtube.com/watch?v=JB8T_zN7ZC0&t=251s)\n",
    "\n",
    "\n",
    "For learning more about batch normalization, I recommend checking out the links below: \n",
    "\n",
    "1. [Batch Norm Explained Visually  How it works, and why neural networks need it](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)\n",
    "2. [Why does batch norm uses exponentially weighted average (EWA) instead of simple average at test time?](https://stats.stackexchange.com/questions/539798/why-does-batch-norm-uses-exponentially-weighted-average-ewa-instead-of-simple)\n",
    "3. [Why does batch normalization use mini-batch statistics instead of the moving averages during training?](https://stats.stackexchange.com/questions/283641/why-does-batch-normalization-use-mini-batch-statistics-instead-of-the-moving-ave)\n",
    "4. [Why perform batch norm before ReLu and not after?](https://forums.fast.ai/t/why-perform-batch-norm-before-relu-and-not-after/81293/4)\n",
    "5. [Intro to Optimization in Deep Learning: Busting the Myth About Batch Normalization](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n",
    "6. [Batch normalization in 3 levels of understanding](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)\n",
    "7. [Moving average in Batch Normalization](https://jiafulow.github.io/blog/2021/01/29/moving-average-in-batch-normalization/)\n",
    "8. [Why does batch normalization enable higher learning rate?](https://www.quora.com/Why-does-batch-normalization-enable-higher-learning-rate)\n",
    "9. [Does Batch Normalized network still need scaled inputs?](https://stats.stackexchange.com/questions/485940/does-batch-normalized-network-still-need-scaled-inputs)\n",
    "10. [is scaling data [0,1] necessary when batch normalization is used?](https://stats.stackexchange.com/questions/249378/is-scaling-data-0-1-necessary-when-batch-normalization-is-used)\n",
    "11. [Understanding the Math behind Batch-Normalization algorithm](https://medium.com/analytics-vidhya/understanding-the-math-behind-batch-normalization-algorithm-part-1-ad5948631ab7)\n",
    "12. [Moving Mean and Moving Variance In Batch Normalization](https://kaixih.github.io/batch-norm/)\n",
    "13. [Does BatchNormalization use moving average across batches or only per batch? and how to use moving average across batches?](https://stackoverflow.com/questions/60460338/does-batchnormalization-use-moving-average-across-batches-or-only-per-batch-and)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b2b9c",
   "metadata": {
    "id": "4c0d71cd"
   },
   "source": [
    "## General References\n",
    "\n",
    "The references below are the main sources I used for preparing the notebooks in this repository. \n",
    "\n",
    "1. Murphy, K. P. (2022). Probabilistic machine learning: an introduction. MIT press.\n",
    "2. Gron, A. (2022). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O'Reilly Media, Inc.\".\n",
    "3. Chollet, F. (2021). Deep learning with Python. Simon and Schuster.\n",
    "4. Kar, K. (2020). Mastering Computer Vision with TensorFlow 2. x: Build advanced computer vision applications using machine learning and deep learning techniques. Packt Publishing Ltd.\n",
    "5. Planche, B., & Andres, E. (2019). Hands-On Computer Vision with TensorFlow 2: Leverage deep learning to create powerful image processing apps with TensorFlow 2.0 and Keras. Packt Publishing Ltd.\n",
    "6. Lakshmanan, V., Grner, M., & Gillard, R. (2021). Practical Machine Learning for Computer Vision. \" O'Reilly Media, Inc.\"."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
